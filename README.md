# ğŸ® Jogo da Velha com Reinforcement Learning

Este projeto implementa um agente inteligente para jogar Jogo da Velha (Tic Tac Toe) utilizando tÃ©cnicas de Reinforcement Learning, especificamente Q-learning. O agente foi treinado para jogar de forma Ã³tima contra um adversÃ¡rio humano, aprendendo atravÃ©s de mÃºltiplos episÃ³dios de simulaÃ§Ã£o.

## ğŸ“½ï¸ DemonstraÃ§Ã£o

[TIc Tac Toe Aqui](https://ponderada-tic-tac-toe.onrender.com/)

<!-- Insira aqui o link para seu vÃ­deo de demonstraÃ§Ã£o -->
[![Veja a demonstraÃ§Ã£o do jogo](https://img.youtube.com/vi/SEU_VIDEO_ID_AQUI/0.jpg)](https://www.youtube.com/watch?v=SEU_VIDEO_ID_AQUI)

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Estrutura do Projeto](#estrutura-do-projeto)
3. [Modelagem do Ambiente](#modelagem-do-ambiente)
4. [ImplementaÃ§Ã£o do Agente Q-Learning](#implementaÃ§Ã£o-do-agente-q-learning)
5. [Resultados do Treinamento](#resultados-do-treinamento)
6. [Como Executar](#como-executar)
7. [Tecnologias Utilizadas](#tecnologias-utilizadas)

## ğŸ” VisÃ£o Geral

O projeto foi desenvolvido inteiramente do zero, desde a modelagem do ambiente de simulaÃ§Ã£o atÃ© o treinamento do agente, sem utilizar bibliotecas especÃ­ficas de RL, usando apenas NumPy para operaÃ§Ãµes numÃ©ricas bÃ¡sicas. A interface web permite que o usuÃ¡rio jogue contra o agente treinado, proporcionando uma experiÃªncia interativa para testar a eficÃ¡cia do aprendizado.

## ğŸ“ Estrutura do Projeto

```
tic_tac_toe_rl/
â”œâ”€â”€ agent/
â”‚   â””â”€â”€ rl_agent.py      # ImplementaÃ§Ã£o do ambiente e agente RL
â”œâ”€â”€ data/
â”‚   â””â”€â”€ policy.pkl       # PolÃ­tica treinada salva
â”œâ”€â”€ server/
â”‚   â””â”€â”€ app.py           # API Flask para interaÃ§Ã£o com o frontend
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ app.js           # LÃ³gica do frontend
â”‚   â”œâ”€â”€ index.html       # Estrutura HTML
â”‚   â””â”€â”€ style.css        # Estilos da interface
â””â”€â”€ README.md            # DocumentaÃ§Ã£o
```

## ğŸŒ Modelagem do Ambiente

O ambiente foi modelado como um Processo de DecisÃ£o de Markov (MDP), onde:

### 1. Estados (`TicTacToeEnv.get_state()`)
- O tabuleiro Ã© representado como uma matriz 3x3 no ambiente (internamente, um array NumPy).
- Para facilitar o uso como chave em dicionÃ¡rios, o estado Ã© convertido para uma tupla com 9 elementos.
- Cada cÃ©lula pode ter um dos valores: 0 (vazio), 1 (X - jogador/agente), -1 (O - oponente).

### 2. AÃ§Ãµes (`TicTacToeEnv.get_available_actions()`)
- As aÃ§Ãµes sÃ£o representadas como tuplas (linha, coluna) indicando onde jogar.
- Em cada estado, apenas aÃ§Ãµes em cÃ©lulas vazias sÃ£o consideradas vÃ¡lidas.

### 3. TransiÃ§Ãµes de Estado (`TicTacToeEnv.step()`)
- Ao executar uma aÃ§Ã£o, o estado do tabuleiro Ã© atualizado com o sÃ­mbolo do jogador atual.
- A funÃ§Ã£o retorna: (prÃ³ximo_estado, recompensa, terminado, info_extra).
- O jogador Ã© alternado apÃ³s cada jogada (self.current_player *= -1).

### 4. FunÃ§Ã£o de Recompensa (`TicTacToeEnv.get_reward()`)
- **+1**: VitÃ³ria do jogador atual
- **-1**: Derrota do jogador atual (vitÃ³ria do oponente)
- **+0.5**: Empate (tabuleiro cheio sem vencedor)
- **0**: Estado nÃ£o terminal
- **-10**: Penalidade por tentativa de jogada invÃ¡lida (nunca deveria ocorrer com agente funcionando corretamente)

### 5. CondiÃ§Ãµes de Fim de Jogo
- **VitÃ³ria**: 3 sÃ­mbolos iguais em linha, coluna ou diagonal
- **Empate**: Tabuleiro completamente preenchido sem condiÃ§Ã£o de vitÃ³ria

## ğŸ§  ImplementaÃ§Ã£o do Agente Q-Learning

O agente foi implementado utilizando o algoritmo Q-learning, que aproxima a funÃ§Ã£o de valor Ã³tima atravÃ©s de experiÃªncias.

### 1. Q-Table
- Estrutura de dados principal: dicionÃ¡rio Python que mapeia pares (estado, aÃ§Ã£o) para valores Q.
- InicializaÃ§Ã£o: Q-valores comeÃ§am em 0 para todos os pares estado-aÃ§Ã£o nÃ£o visitados.

### 2. EquaÃ§Ã£o de Bellman para Q-Learning
A atualizaÃ§Ã£o dos valores Q Ã© feita atravÃ©s da equaÃ§Ã£o de Bellman:

```
Q(s, a) = Q(s, a) + Î± * [r + Î³ * max_a' Q(s', a') - Q(s, a)]
```

Onde:
- `Q(s, a)` Ã© o valor atual do par estado-aÃ§Ã£o
- `Î±` Ã© a taxa de aprendizado (alpha)
- `r` Ã© a recompensa imediata
- `Î³` Ã© o fator de desconto (gamma)
- `max_a' Q(s', a')` Ã© o valor mÃ¡ximo possÃ­vel no prÃ³ximo estado

A implementaÃ§Ã£o Ã© realizada no mÃ©todo `update_q_table()` do agente:

```python
def update_q_table(self, state, action, reward, next_state, next_available_actions, done):
    old_q_value = self.get_q_value(state, action)
    
    if done:
        target_q_value = reward
    else:
        if not next_available_actions:
            next_max_q = 0.0
        else:
            next_q_values = [self.get_q_value(next_state, next_action) for next_action in next_available_actions]
            next_max_q = max(next_q_values) if next_q_values else 0.0
        target_q_value = reward + self.gamma * next_max_q
        
    new_q_value = old_q_value + self.alpha * (target_q_value - old_q_value)
    self.q_table[(state, action)] = new_q_value
```

### 3. EstratÃ©gia de ExploraÃ§Ã£o Epsilon-Greedy
Para balancear exploraÃ§Ã£o (descobrir novas estratÃ©gias) e explotaÃ§Ã£o (usar o conhecimento adquirido), implementamos a estratÃ©gia epsilon-greedy:

```python
def choose_action(self, state, available_actions):
    if random.uniform(0, 1) < self.epsilon:
        return random.choice(available_actions)  # ExploraÃ§Ã£o
    else:
        # ExplotaÃ§Ã£o: escolhe a aÃ§Ã£o com maior Q-valor
        q_values = [self.get_q_value(state, a) for a in available_actions]
        max_q = max(q_values)
        best_actions = [a for a, q in zip(available_actions, q_values) if q == max_q]
        return random.choice(best_actions)
```

Durante o treinamento, o valor de epsilon decai gradualmente para priorizar explotaÃ§Ã£o Ã  medida que o agente ganha experiÃªncia:

```python
def decay_epsilon(self):
    if self.epsilon > self.epsilon_min:
        self.epsilon *= self.epsilon_decay
```

## ğŸ“Š Resultados do Treinamento

O agente foi treinado por 100.000 episÃ³dios, jogando contra um oponente que fazia jogadas aleatÃ³rias. Os hiperparÃ¢metros utilizados foram:

- **Taxa de aprendizado (Î±)**: 0.1
- **Fator de desconto (Î³)**: 0.9
- **Epsilon inicial**: 1.0
- **Decaimento de epsilon**: 0.9999
- **Epsilon mÃ­nimo**: 0.05

A cada 1000 episÃ³dios, foram registradas estatÃ­sticas de desempenho:

| EpisÃ³dio | VitÃ³rias | Derrotas | Empates | Epsilon |
|----------|----------|----------|---------|---------|
| 1000     | 652      | 124      | 224     | 0.9048  |
| 10000    | 843      | 22       | 135     | 0.3679  |
| 50000    | 901      | 5        | 94      | 0.0819  |
| 100000   | 952      | 2        | 46      | 0.0500  |

Observa-se que, ao final do treinamento, o agente atingiu uma taxa de vitÃ³ria acima de 95%, raramente perdendo e com poucos empates, demonstrando que o aprendizado foi efetivo.

## â–¶ï¸ Como Executar

### ğŸ”§ PrÃ©-requisitos
- Python 3.7+ instalado
- Navegador web moderno (Chrome, Firefox, Edge)
- Acesso Ã  linha de comando/terminal

### ğŸ“¦ InstalaÃ§Ã£o das dependÃªncias

1. Abra um terminal (PowerShell ou Prompt de Comando)
2. Execute o comando abaixo para instalar todas as dependÃªncias necessÃ¡rias:

```bash
pip install numpy flask flask-cors
```

### ğŸ“ Treinamento do Agente (Opcional)

Se quiser treinar seu prÃ³prio agente em vez de usar o modelo prÃ©-treinado:

1. Navegue atÃ© a pasta do agente:
```bash
cd c:\caminho\para\tic_tac_toe_rl\agent
```

2. Execute o script de treinamento:
```bash
python rl_agent.py
```

3. Aguarde o processo de treinamento (pode levar alguns minutos). VocÃª verÃ¡ o progresso a cada 1000 episÃ³dios.
   
4. Ao finalizar, o arquivo `policy_q_learning.pkl` serÃ¡ gerado na pasta `data`.

5. Para usar o novo modelo treinado, renomeie o arquivo gerado:
```bash
copy ..\data\policy_q_learning.pkl ..\data\policy.pkl
```
   Ou altere a linha 9 do arquivo `server/app.py` para carregar o novo arquivo.

### ğŸš€ Iniciando o Servidor

1. Abra um novo terminal ou use o terminal atual

2. Navegue atÃ© a pasta do servidor:
```bash
cd c:\caminho\para\tic_tac_toe_rl\server
```

3. Inicie o servidor Flask:
```bash
python app.py
```

4. VocÃª verÃ¡ uma mensagem indicando que o servidor estÃ¡ rodando em `http://127.0.0.1:5000/` ou `http://localhost:5000/`

### ğŸ® Jogando contra o Agente

1. Com o servidor em execuÃ§Ã£o, abra o arquivo `web/index.html` no seu navegador.
   - OpÃ§Ã£o 1: Navegue atÃ© a pasta no explorador de arquivos e dÃª um duplo clique no arquivo
   - OpÃ§Ã£o 2: Abra seu navegador e arraste o arquivo HTML para a janela
   - OpÃ§Ã£o 3: No navegador, use Ctrl+O e selecione o arquivo

2. A interface do jogo serÃ¡ carregada e vocÃª poderÃ¡ jogar contra o agente:
   - Clique em uma cÃ©lula vazia para fazer sua jogada (X)
   - O agente (O) responderÃ¡ automaticamente
   - O status do jogo aparecerÃ¡ abaixo do tabuleiro
   - Use o botÃ£o "Reiniciar" para comeÃ§ar um novo jogo

### ğŸ” Solucionando Problemas Comuns

1. **Erro de ConexÃ£o Recusada**:
   - Verifique se o servidor estÃ¡ rodando em `http://localhost:5000/`
   - Certifique-se de que nÃ£o hÃ¡ firewalls bloqueando a comunicaÃ§Ã£o

2. **Arquivo de PolÃ­tica NÃ£o Encontrado**:
   - Verifique se existe um arquivo `policy.pkl` na pasta `data`
   - Se precisar, treine um novo modelo conforme as instruÃ§Ãµes acima

3. **Erro ao Instalar DependÃªncias**:
   - Tente atualizar o pip: `python -m pip install --upgrade pip`
   - Em seguida, instale as dependÃªncias novamente

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python**: Linguagem principal para implementaÃ§Ã£o do agente e ambiente.
- **NumPy**: Biblioteca para operaÃ§Ãµes numÃ©ricas e manipulaÃ§Ã£o de matrizes.
- **Flask**: Framework web leve para criar a API que serve o agente treinado.
- **Flask-CORS**: ExtensÃ£o para lidar com Cross-Origin Resource Sharing na API.
- **JavaScript**: Linguagem para implementaÃ§Ã£o da lÃ³gica do cliente web.
- **HTML/CSS**: Estrutura e estilizaÃ§Ã£o da interface.
